---
title: Worksheet 9 (2023-11-02)
draft: false
date: 2023-11-02T19:46:09.576Z
summary: " "
joplinId: 72f453dea99143dc80b6efc3604eb0a0
backlinks:
  - id: 72f453dea99143dc80b6efc3604eb0a0
    text: Worksheet 9 (2023-11-02)
    icon: fas
    href: /./worksheet-9-(2023-11-02)
---

# Worksheet 9

Some additional notes from class.

- We're introducing new metrics to use to approximate the test error (i.e., $R^2$ on the test set)
- We can also perform "forward selection" to iterate on models
  - It's a greedy approach; add one variable, determine best model, hold that fixed and repeat
  - `regsubsets` from the `leaps` library in R
  - Can take `nvmax`, the maximum number of variables to consider
  - `tidymodels` alternative in the notebook, too
- CP, try to minimize
- AIC (or... BIC?), similar
- Both estimate test predictive performance; good for prediction models
- Forward, backward selection bad with categorical variables
  - One-hot encoding turns a feature into multiple columns
  - We want to consider 'all' or 'none', not only some categories
- Similar concerns with categorical variables and heatmaps
  - Some visualizations better for categorical variables
